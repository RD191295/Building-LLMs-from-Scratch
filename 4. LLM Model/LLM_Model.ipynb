{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "942f4ea1",
   "metadata": {},
   "source": [
    "### GPT-2 Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f17665bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT COnfiguration\n",
    "GPT_CONFIG_124M ={\n",
    "    \"Vocab_size\": 50527,    # Vocabulary size\n",
    "    \"context_length\": 256, # Context length\n",
    "    \"emb_dim\":768,          # Embedding dimension\n",
    "    \"n_heads\":12,           # Number of attention heads\n",
    "    \"n_layers\":12,          # NUmber of layers\n",
    "    \"drop_rate\":0.1,         # Dropout rate\n",
    "    \"qkv_bias\":False        # Query-key-value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8d72ed",
   "metadata": {},
   "source": [
    "### GPT ARCHITECTURE : DUMMY GPT MODEL CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1c91e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a05bfe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy GPT Model\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"Vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        # Use a placeholder for Transformer block\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        # Use a placeholder for LayerNorm\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"Vocab_size\"] , bias = False\n",
    "        )\n",
    "    \n",
    "    # forward method\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b07a93f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken as tk\n",
    "\n",
    "tokenizer = tk.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 =\"your every effort moves\"\n",
    "txt2 = \"your day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "\n",
    "batch= torch.stack(batch, dim=0)\n",
    "\n",
    "# print shape of batch\n",
    "print(batch.shape) # (2, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f849eef6",
   "metadata": {},
   "source": [
    "### Coding 124-M Parameter GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a94d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        var = x.var(dim= -1, keepdim = True)\n",
    "        norm_x  = (x-mean) / torch.sqrt(var +self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9cbe1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return 0.5 * x *(1+torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0/torch.pi)) *\n",
    "            (x + 0.044715*torch.pow(x,3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e77f9169",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4*cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4*cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e60f1b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in :int,d_out:int,num_of_heads:int,\n",
    "                 context_length:int,dropout:float,qkv_bias:bool=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        assert (d_out % num_of_heads == 0),\\\n",
    "              \"d_out is not divisible by num_of_heads\"\n",
    "        self.head_dim = d_out // num_of_heads\n",
    "        self.num_of_heads = num_of_heads\n",
    "        self.w_query = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.w_Key = nn.Linear(d_in,d_out,bias=qkv_bias) \n",
    "        self.w_value = nn.Linear(d_in,d_out,bias=qkv_bias) \n",
    "        self.projLayer = nn.Linear(d_out,d_out,bias=qkv_bias) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length,context_length),\n",
    "                       diagonal=1)\n",
    "        )   \n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        b, num_of_token , d_in = x.shape\n",
    "        \n",
    "        Query = self.w_query(x) \n",
    "        Key = self.w_Key(x) \n",
    "        Value = self.w_value(x) \n",
    "\n",
    "        #(b,num_of_token,d_in) ---> (b,num_of_token,num_of_heads,head_dim) \n",
    "        Query = Query.view(b,num_of_token,self.num_of_heads,self.head_dim)\n",
    "        Key = Key.view(b,num_of_token,self.num_of_heads,self.head_dim)\n",
    "        Value = Value.view(b,num_of_token,self.num_of_heads,self.head_dim)\n",
    "\n",
    "        # Grouping By heads ( transpose 1 and 2 shape)\n",
    "        #shape : (b,num_of_heads,num_of_token,head_dim) \n",
    "        Query = Query.transpose(1,2)\n",
    "        Key = Key.transpose(1,2)\n",
    "        Value = Value.transpose(1,2)\n",
    "\n",
    "        # attaention score\n",
    "        atten_score  = Query @ Key.transpose(2,3)\n",
    "\n",
    "        # create mask\n",
    "        mask_bool  = self.mask.bool()[:num_of_token,:num_of_token]\n",
    "\n",
    "        #apply mask\n",
    "        atten_score = atten_score.masked_fill(\n",
    "                            mask_bool,\n",
    "                            -torch.inf\n",
    "                        )\n",
    "\n",
    "        # apply softmax\n",
    "        attn_weight = torch.softmax(\n",
    "            atten_score / Key.shape[-1] ** 0.5 , dim = -1\n",
    "        )\n",
    "\n",
    "        #context vector\n",
    "        context_vec  = (attn_weight @ Value).transpose(1,2)\n",
    "\n",
    "        #Merge Head_dim (b,num_of_token,num_of_heads,head_dim) --> (b,num_of_token,d_out)\n",
    "        context_vec = context_vec.contiguous().view(b,num_of_token,self.d_out)\n",
    "\n",
    "        # apply projection layer\n",
    "        context_vec = self.projLayer(context_vec)\n",
    "        \n",
    "        # apply dropout and return\n",
    "        return self.dropout(context_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "549016a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_in=cfg[\"emb_dim\"],\n",
    "                                            d_out=cfg[\"emb_dim\"],\n",
    "                                            num_of_heads=cfg[\"n_heads\"],\n",
    "                                            context_length=cfg[\"context_length\"],\n",
    "                                            dropout = cfg[\"drop_rate\"],\n",
    "                                            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "\n",
    "        self.feedforward = FeedForward(cfg)\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x \n",
    "        x = self.norm1(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.feedforward(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b96d358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy GPT Model\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"Vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        # Use a placeholder for Transformer block\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        # Use a placeholder for LayerNorm\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"Vocab_size\"] , bias = False\n",
    "        )\n",
    "    \n",
    "    # forward method\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c72269e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8092bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initalize Dummpy GPT Class\n",
    "import tiktoken as tk\n",
    "\n",
    "tokenizer = tk.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 =\"your every effort moves\"\n",
    "txt2 = \"your day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "\n",
    "batch= torch.stack(batch, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "818e2d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Batch:\n",
      " tensor([[14108,   790,  3626,  6100],\n",
      "        [14108,  1110,  6622,   257]])\n",
      "Input shape:\n",
      " torch.Size([2, 4])\n",
      "Output shape:\n",
      " torch.Size([2, 4, 50527])\n",
      "tensor([[[ 0.2335, -0.0799,  0.4891,  ...,  0.7267,  0.0185,  0.1434],\n",
      "         [ 0.2745,  0.3663, -0.0066,  ..., -0.1622,  0.6408,  0.5155],\n",
      "         [ 0.7870, -0.8251,  0.6020,  ..., -0.6042,  0.3742,  0.4305],\n",
      "         [-0.2272, -0.4116,  0.2539,  ..., -0.8637,  0.1731, -0.0040]],\n",
      "\n",
      "        [[ 0.2679, -0.2616,  0.3264,  ...,  0.5212,  0.0633,  0.7434],\n",
      "         [ 0.2792, -0.1173,  0.4353,  ...,  0.7961,  0.0773,  1.0298],\n",
      "         [ 0.4440,  0.1749, -0.3539,  ..., -0.4682, -0.6033,  0.1506],\n",
      "         [-0.0408, -0.2424,  0.3245,  ..., -0.4561,  0.0220,  0.6345]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = model(batch)\n",
    "print(\"Input Batch:\\n\",batch)\n",
    "print(\"Input shape:\\n\",batch.shape)\n",
    "print(\"Output shape:\\n\",out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d65fa03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameter is:163,415,040\n"
     ]
    }
   ],
   "source": [
    "# Total parameter count\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameter is:{total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "169f31b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Embedding layer shape: torch.Size([50527, 768])\n",
      "Output layer shape: torch.Size([50527, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token Embedding layer shape:\",model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\",model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c2ee2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameter in GPT 2 is:124,610,304\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 = total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "print(f\"Total number of parameter in GPT 2 is:{total_params_gpt2:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cff0535e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total size of the model:623.38 MB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_params * 4\n",
    "total_size_mb = total_size_bytes / (1024 *1024)\n",
    "print(f\"total size of the model:{total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52be965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model:GPTModel, idx:list[int], max_new_tokens:int,context_size:int):\n",
    "    for _ in range(max_new_tokens):\n",
    "        # get last token size of context size\n",
    "        idx_cond = idx[:,-context_size:]\n",
    "\n",
    "        # get output\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # get last raw from logits tensor\n",
    "        logits = logits[:,-1,:]\n",
    "\n",
    "        # apply softmax\n",
    "        probs = torch.softmax(logits,dim=-1)\n",
    "\n",
    "        ids_next = torch.argmax(probs, dim = -1, keepdim=True)\n",
    "\n",
    "        idx = torch.cat((idx, ids_next), dim = 1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12de5715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encode:\",encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape\",encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52cad76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 42009, 18283, 37256, 10358, 28640, 24274]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "out = generate_text_simple(model = model,\n",
    "                           idx = encoded_tensor,\n",
    "                           max_new_tokens=6,\n",
    "                           context_size=GPT_CONFIG_124M['context_length'])\n",
    "print(\"Output:\",out)\n",
    "print(\"Output length:\",len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30d31141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I amlectic trackedacons Should inflictInternational\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist()) # type: ignore\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f387301b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you anywhere sovere fat oct Swordsman Supportsycle Cron virtueism\n"
     ]
    }
   ],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text , allowed_special={\"<|endoftext|>\"})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model = model ,\n",
    "    idx = text_to_token_ids(start_context,tokenizer),\n",
    "    max_new_tokens= 10,\n",
    "    context_size= GPT_CONFIG_124M['context_length']\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids=token_ids, tokenizer= tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ecb6eff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833,3626,6100],\n",
    "                       [40,1107,588]])\n",
    "\n",
    "targets = torch.tensor([[3626,6100,345],\n",
    "                        [1107,588,11311]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b9f0fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50527])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probs = torch.softmax(logits, dim = -1)\n",
    "print(probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1b2c58d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[27728],\n",
       "         [44487],\n",
       "         [28687]],\n",
       "\n",
       "        [[25797],\n",
       "         [21714],\n",
       "         [ 5485]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = torch.argmax(probs, dim = -1, keepdim=True)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a2ddb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traget Batch 1: effort moves you\n",
      "Ouputs Batch 1:298 Sidd 370\n"
     ]
    }
   ],
   "source": [
    "print(f\"Traget Batch 1:{token_ids_to_text(targets[0],tokenizer)}\")\n",
    "print(f\"Ouputs Batch 1:{token_ids_to_text(token_ids[0].flatten(),tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaf6a55",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7507679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target 0 prob:\n",
      " tensor([1.0570e-05, 8.3353e-06, 2.2679e-05])\n",
      "target 1 prob:\n",
      " tensor([9.4317e-06, 4.3987e-06, 6.1416e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "\n",
    "target_probabs_0 = probs[text_idx, [0,1,2], targets[text_idx]]\n",
    "print(\"target 0 prob:\\n\",target_probabs_0)\n",
    "\n",
    "text_idx = 1\n",
    "target_probabs_1 = probs[text_idx, [0,1,2], targets[text_idx]]\n",
    "print(\"target 1 prob:\\n\",target_probabs_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "decd22e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-11.4575, -11.6950, -10.6941, -11.5714, -12.3342, -12.0004])\n"
     ]
    }
   ],
   "source": [
    "log_probs = torch.log(torch.cat((target_probabs_0,target_probabs_1)))\n",
    "print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8d592e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-11.6254)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probs = torch.mean(log_probs)\n",
    "print(avg_log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f221e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.6254)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probs = avg_log_probs * -1\n",
    "print(neg_avg_log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0fc2d193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flatterned logits: torch.Size([6, 50527])\n",
      "Flatterned targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0,1)\n",
    "\n",
    "targets_flat = targets.flatten(0,1)\n",
    "\n",
    "print(\"Flatterned logits:\",logits_flat.shape)\n",
    "\n",
    "print(\"Flatterned targets:\",targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab3a34e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.6254)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.cross_entropy(logits_flat,targets_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e4e47a",
   "metadata": {},
   "source": [
    "### Perplexity\n",
    "\n",
    "\n",
    "#### Measure how well the probability distribution predicted by the model matches the actual distribution of words in dataset\n",
    "\n",
    "#### More interetable way of understanding model uncertanity in pred next token\n",
    "\n",
    "#### lower score ---> better score\n",
    "\n",
    "#### preplexity  = torch.exp(corss entropy loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "76636a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(111909.0234)\n"
     ]
    }
   ],
   "source": [
    "perplexity_loss = torch.exp(torch.nn.functional.cross_entropy(logits_flat,targets_flat))\n",
    "print(perplexity_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2152c3",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "06c4ff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'D:\\Data_Science_Study\\Course_Pracitse_Code\\GenerativeAi\\LLM_From_Scratch\\Building-LLMs-from-Scratch\\0. Data\\the-verdict.txt') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9c150772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art.\"\n"
     ]
    }
   ],
   "source": [
    "#last 100 char\n",
    "print(data[-99:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fe4536c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(data)\n",
    "total_tokens = len(tokenizer.encode(data))\n",
    "\n",
    "print(\"Characters:\",total_characters)\n",
    "print(\"Tokens:\",total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "23a73b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self,txt, tokenizer,max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"|<endoftext>|\"})\n",
    "\n",
    "\n",
    "        for i in range(0,len(token_ids)-max_length,stride):\n",
    "            input_chunks = token_ids[i:i+max_length]\n",
    "            target_chunks = token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunks))\n",
    "            self.target_ids.append(torch.tensor(target_chunks))\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx] # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9e1b6dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1e837c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(text,batch_size=4,max_length =256,\n",
    "                        stride = 128, shuffle = True, drop_last = True,\n",
    "                        num_workers = 0):\n",
    "\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "    # create dataset\n",
    "    dataset = GPTDatasetV1(text, tokenizer,max_length, stride)\n",
    "\n",
    "    # create dataloader\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size = batch_size,\n",
    "        shuffle = shuffle,\n",
    "        drop_last = drop_last,\n",
    "        num_workers = num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f762a6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(data))\n",
    "train_data = data[:split_idx]\n",
    "val_data = data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2f666e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride = GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4309371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride = GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c1904e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if total_tokens *(train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokrns for the train loader\"\n",
    "          \"Try with more data\")\n",
    "    \n",
    "if total_tokens *(1- train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokrns for the train loader\"\n",
    "          \"Try with more data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "46588093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation Loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Loader:\")\n",
    "for x , y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation Loader:\")\n",
    "for x , y in val_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "\n",
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f8cf792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch,target_batch, model ,device):\n",
    "    input_batch ,target_batch = input_batch.to(device),target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0,1),target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(data_loader,model, device,num_batches =None):\n",
    "    total_loss = 0\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    \n",
    "\n",
    "    for i , (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i <num_batches:\n",
    "            loss = calc_loss_batch(input_batch,target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return total_loss/num_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f647fb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b700a8d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x21d9d15a210>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "af8d0391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 10.966801431443956\n",
      "validation loss: 10.966813087463379\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader,model,device)\n",
    "    val_loss = calc_loss_loader(val_loader,model, device)\n",
    "\n",
    "print(\"training loss:\", train_loss)\n",
    "print(\"validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a5c600",
   "metadata": {},
   "source": [
    "### Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555ebddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6])\n",
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(123) # type: ignore\n",
    "\n",
    "batch_example = torch.randn(2,5)\n",
    "\n",
    "layer = nn.Sequential(nn.Linear(5,6),nn.ReLU())\n",
    "\n",
    "out = layer(batch_example)\n",
    "\n",
    "print(out.shape) # (2, 6)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76144a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim= -1, keepdim=True)\n",
    "var = out.var(dim= -1, keepdim=True)\n",
    "\n",
    "print(\"Mean:\\n\",mean)\n",
    "print(\"Variance:\\n\",var) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aebd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Output:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean of Normalized Output:\n",
      " tensor([[9.9341e-09],\n",
      "        [0.0000e+00]], grad_fn=<MeanBackward1>)\n",
      "Variance of Normalized Output:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean)/torch.sqrt(var)\n",
    "\n",
    "mean_norm = out_norm.mean(dim= -1, keepdim=True)\n",
    "var_norm = out_norm.var(dim= -1, keepdim=True)\n",
    "\n",
    "print(\"Normalized Output:\\n\",out_norm)\n",
    "print(\"Mean of Normalized Output:\\n\",mean_norm)\n",
    "print(\"Variance of Normalized Output:\\n\",var_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c84b8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Normalized Output:\n",
      " tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance of Normalized Output:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode = False)\n",
    "print(\"Mean of Normalized Output:\\n\",mean_norm)\n",
    "print(\"Variance of Normalized Output:\\n\",var_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e27431",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1 ,keepdim = True)\n",
    "        var = x.var(dim = -1, keepdim = True, unbiased =False)\n",
    "        norm_x = (x-mean)/ torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2b4c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out \n",
      " tensor([[ 0.5528,  1.0693, -0.0223,  0.2656, -1.8654],\n",
      "        [ 0.9087, -1.3767, -0.9564,  1.1304,  0.2940]], grad_fn=<AddBackward0>)\n",
      "Mean \n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance \n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "\n",
    "out_ln = ln(batch_example)\n",
    "\n",
    "mean_ln = out_ln.mean(dim = -1,keepdim = True)\n",
    "var_ln = out_ln.var(dim = -1,keepdim= True,unbiased =False)\n",
    "\n",
    "print(\"out \\n\", out_ln)\n",
    "\n",
    "print(\"Mean \\n\", mean_ln)\n",
    "\n",
    "print(\"Variance \\n\", var_ln)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
